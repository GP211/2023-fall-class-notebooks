{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bae9747-f8e5-4f9f-8b65-09727c273b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GP211/2023-fall-class-notebooks/blob/main/in-class/012-Seabin.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb17d14-03e2-4e00-b58f-b297b1f771f9",
   "metadata": {},
   "source": [
    "# Seabeam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25cf7e-4c85-4243-8e0f-47e3b97b5069",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "Lets begin by loading our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5746d-7195-401a-813d-19da1ee0124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "!python3 -m pip install \"giee @ git+https://github.com/GP211/2023-fall-class-notebooks.git@d00084220a6501d5fe744869f82e64b3dab9c03b\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02804f63-2f31-45fb-9722-951f130d6a31",
   "metadata": {},
   "source": [
    "Don't forget to restart your runtime before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c1457-8462-41d6-b9ec-64b403673550",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a5fa8-e79d-4df3-b610-b86d1a80ce41",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following shows an image of deep seawater bottom\n",
    "in the Pacific of a sea-floor spreading center produced acoustically\n",
    "by a device called SeaBeam. Note how we have only acquired data over\n",
    "a limited range making this an ideal missing data problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b91246-1d33-43aa-9c79-fe1c9ab8d5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sep_python import default_io, Hypercube, FloatVector\n",
    "import numpy as np\n",
    "from sep_plot import Grey\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh','matplotlib')\n",
    "!wget https://raw.githubusercontent.com/rgclapp007/gp211-class-notebooks/main/data/seabin.HH\n",
    "\n",
    "vec=default_io.vector_from_storage(\"./seabin.HH\")\n",
    "Grey(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71d4f5-108e-4a6e-a91b-71e7358b4843",
   "metadata": {},
   "source": [
    "## Estimation with a second derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a4d09-a17d-47f4-86a7-2f4f0ee1e473",
   "metadata": {},
   "source": [
    "As a first attempt we are going to use the second derivative to fill in the missing data. We are going to set this problem up by first binning the data to a regular gird. Our data fitting goal will then be to make sure the model matches the data at the known locations through the operator $\\bf J$. Our model styling goal will assume the model has symetric covariance. We will therefore apply a Laplaian ($\\bf L$)\n",
    "$O(\\bf m) =||\\bf d - \\bf J \\bf m||^2 + \\epsilon^2 || \\bf L \\bf m||^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926c68a-f949-4c65-8b98-f0ea12d747bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from giee import BoxFilter, ConvOpAdjFilter, convOpAdjData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fb107-2a51-4e66-bcab-70b1a51339a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from generic_solver._pyOperator import Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e9688-a017-4afb-a1ab-c673c84e177e",
   "metadata": {},
   "source": [
    "Lets begin by writing a function that determines where we have data samples.  We will return two numpy arrays that we will turn into sep vectors, one containing a map containing where we have samples set to 1, other set to 1 where we don't have valies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef90014-8b05-4d41-a824-0ae13528b21e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sep_python import get_sep_vector\n",
    "from numba import njit\n",
    "hv.extension('bokeh','matplotlib')\n",
    "\n",
    "@njit()\n",
    "def known_data(vec, unknown_value=0):\n",
    "    known=np.copy(vec)\n",
    "    unknown=np.copy(vec)\n",
    "    for i2 in range(vec.shape[0]):\n",
    "        for i1 in range(vec.shape[1]):\n",
    "            if vec[i2,i1]==unknown_value:\n",
    "                known[i2,i1]=0\n",
    "                unknown[i2,i1]=1\n",
    "            else:\n",
    "                known[i2,i1]=1\n",
    "                unknown[i2,i1]=0\n",
    "    return known, unknown\n",
    "\n",
    "k,u=known_data(vec.get_nd_array())\n",
    "known=get_sep_vector(k,hyper=vec.get_hyper())\n",
    "unknown=get_sep_vector(u,hyper=vec.get_hyper())\n",
    "\n",
    "Grey(known)+Grey(unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f8dd0-d9e5-43e3-b8cb-d509274789ed",
   "metadata": {},
   "source": [
    "Next lets create are operators $\\bf J$ and $\\bf L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ec613-b938-4068-abd9-92f29f50e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generic_solver._pyOperator import DiagonalOp\n",
    "from giee import Lap2D\n",
    "j_op=DiagonalOp(unknown)\n",
    "reg=Lap2D(vec,vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1e0d9-922b-4135-b257-247061ef3639",
   "metadata": {},
   "source": [
    "Lets solve are simple regularized inversion problem and display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345a985-a6b8-44fc-b5ec-d6a97ed5a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generic_solver  import ProblemL2Linear,ProblemL2LinearReg, LCGsolver, BasicStopper\n",
    "hv.extension('bokeh','matplotlib')\n",
    "\n",
    "data=vec.clone()\n",
    "model=data.clone()\n",
    "eps=100\n",
    "\n",
    "problemStop=BasicStopper(niter=1000)\n",
    "problemLap1=ProblemL2LinearReg(model,data,j_op,eps,reg_op=reg)\n",
    "solve_base=LCGsolver(problemStop)\n",
    "solve_base.run(problemLap1)\n",
    "\n",
    "Grey(problemLap1.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2580cdc8-8883-41a3-9635-b8fffee93bf6",
   "metadata": {},
   "source": [
    "## Problems with our covariance description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e97d3-75ca-4635-805b-40cf8b3ba680",
   "metadata": {},
   "source": [
    "Generally not a very satisfactory result.  The problem\n",
    "is that a second derivative is not an accurate description of\n",
    "the model covariance.\n",
    "\n",
    "We can get an idea of the covariance of our problem by averaging over samples\n",
    "some vector difference away. As a result we get a plot where the axes are now\n",
    "vector differences and the amplitude tells us about covariance between the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2148f-12ae-48d0-be42-fcadef680e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sep_python import get_sep_vector\n",
    "hv.extension('bokeh','matplotlib')\n",
    "from numba import njit\n",
    "@njit()\n",
    "def covFunc(model,nm1,nm2,cov):\n",
    "    for i2 in range(model.shape[0]):\n",
    "        for i1 in range(model.shape[1]):\n",
    "            for im2 in range(-nm2,nm2):\n",
    "                if i2+im2 >=0 and i2+im2 < model.shape[0]:\n",
    "                    for im1 in range(-nm1,nm1):\n",
    "                        if i1+im1 >=0 and i1+im1 < model.shape[1]:\n",
    "                            cov[im2+nm2,im1+nm1]+=(model[i2,i1]*model[i2+im2,i1+im1])\n",
    "cov=get_sep_vector(ns=[81,81],os=[-40,-40],ds=[1,1])\n",
    "hyper=Hypercube.set_with_ns([81,81],os=[-40,-40],ds=[1,1])\n",
    "covFunc(vec.getNdArray(),40,40,cov.get_nd_array())\n",
    "Grey(cov,pclip=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17974a7b-d62b-4ecb-a9db-b8dacbca587b",
   "metadata": {},
   "source": [
    "Note how we the covariance is not as simple as a second derivative.\n",
    "We see several different linear-like trends. The most dominant dipping slightly down as\n",
    "we move left to right in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d316d2-29d1-4dc4-aa61-6e2d2db5c2f5",
   "metadata": {},
   "source": [
    "## Prediction Error Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b869fb-4924-49f4-b996-8510dbd71ee7",
   "metadata": {},
   "source": [
    "We could try to redesign our regularization operator to emulate that trend but a better solution is to use Prediction Error Filters (PEFs) to capture the inverse spectrum.\n",
    "\n",
    "In general, with Prediction Error Filters, we are attempting to find the filter $\\bf f$ that when convolved with the data $\\bf D$ gives a minimum energy solution, which would seem to lead to\n",
    "$O(\\bf f) = || \\bf D \\bf f||^2$. With our filter, we only want to convolve with samples that occur at or before in time (and space). This is what is called a {\\it causal} filter.  \n",
    "\n",
    "The problem is that the obvious answer to this solution is\n",
    "for $\\bf f$ =\\bf 0$.  To address this issue, we are going to fix one of the \n",
    "coefficients in the filter to be 1, specifically the one associated with zero\n",
    "lag in time and space.\n",
    "\n",
    "A second issue is that the minimization equation above isn't correct.  If the data was actually $\\bf 0$, even fixing a coefficient we would still get $\\bf 0$.  What we really want is $O(\\bf f) = || \\bf d + \\bf D \\bf f||^2$, which in terms of the way we have\n",
    "been solving problems is equivalent to $O(\\bf f) = || - \\bf d - \\bf D \\bf f||^2$.\n",
    "\n",
    "As far as how to construct the shape of $\\bf f$.  Remember that the longer $\\bf f$ is in the time axis the wide ranger of dips we can recover and the more complex spectra we can model. The more rows we add to $\\bf f$ the more dips we can capture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478af7b-81ef-4cd6-9a5d-af3c33adc356",
   "metadata": {},
   "source": [
    "For the Seabeam problem, we are further challenged because\n",
    "we can only estimate spectra on locations where\n",
    "we are filter fully sits on the data. Whenever any of our\n",
    "filter coefficients are not on a recorded data sample leading to an inaccurate \n",
    "fitting equation.  As a result, we are going to add a weighting function  $\\bf W$\n",
    "where $\\bf W$ is 0 at any location where one or more coefficients of the\n",
    "the filters are on unknown data samples. Our new minimization becomes\n",
    " $O(\\bf f) = ||\\bf W( - \\bf d - \\bf D \\bf f)||^2$.  To fit within are\n",
    "standard inversion scheme we can define a new data $\\bf d_{new} = - \\bf W \\bf d$ and new operator $\\bf D_new = \\bf W \\bf D$.\n",
    "\n",
    "Below you can find code that estimates a Prediction Error Filter using this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2565204-c72d-43d1-9f15-f9c839d44096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from generic_solver._pyOperator import DiagonalOp, ChainOperator\n",
    "from giee import BoxFilter, ConvOpAdjFilter\n",
    "from sep_python import Hypercube\n",
    "\n",
    "def findPef(data,known,sh,zero):\n",
    "    filt=BoxFilter.PEF(sh,zero)\n",
    "    op=ConvOpAdjFilter(filt,data,data)\n",
    "    wt=filt.create_mask(known)\n",
    "    wtOp=DiagonalOp(wt)\n",
    "    # \n",
    "    #  W(d-Lm) = Wd -WLm\n",
    "    #\n",
    "    duse=data.clone()\n",
    "    data.scale(-1.)\n",
    "    wtOp.forward(False,data,duse)\n",
    "    wt_pef=ChainOperator(op,wtOp)\n",
    "\n",
    "    prob=ProblemL2Linear(filt,duse,wt_pef)\n",
    "    #-d = D f  \n",
    "    stop=BasicStopper(niter=1000)\n",
    "    solve=LCGsolver(stop)\n",
    "    solve.run(prob)\n",
    "    return prob.model,prob.res,wt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efceee-8c2a-4dce-95e9-809fe4b0308c",
   "metadata": {},
   "source": [
    "Note in the above code create_mask which returns the weighting function. The trick applied in that function is to set all of our filter coefficients to 1, then convolve a model of 1s.  Any data values that are equal to the number of filter coefficients is a valid regression equation.\n",
    "\n",
    "We are going to construct a filter which is 7 in the vertical (fast) axis and 4 in the time horizontal axis. Lets first plot the weighting function to see where we have valid equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c87aa-d243-440f-90d3-fa16cb964f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib')\n",
    "\n",
    "pef,res,wt=findPef(vec,known,(4,7),(0,3))\n",
    "Grey(wt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef674dfe-78ad-4df9-a300-8a428a71c62f",
   "metadata": {},
   "source": [
    "Note that the locations where we have valid equations are significantly less than where we have data which makes sense given the filter dimensions.\n",
    "\n",
    "It can also be useful to look at residuals. Note the IID nature of the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36581f-52f9-4aa8-b292-9db3962f3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib')\n",
    "Grey(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6cd7f-2b7e-4c56-8942-4e353c7b0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimating with the Prediction Error Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdecd3-f7a3-4ed6-b8c1-8231eb51f3da",
   "metadata": {},
   "source": [
    "Now that we have the PEF we can use it to regularize are problem.  Let's solve the problem substituting convolving with the  PEF for the Laplacian in our standard regularized inversion problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a40bb-6d2d-45eb-abf6-83dc499ea4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib')\n",
    "from giee import convOpAdjData\n",
    "\n",
    "j_op=DiagonalOp(unknown)\n",
    "problemStop=BasicStopper(niter=1000)\n",
    "data=vec.clone()\n",
    "model=data.clone()\n",
    "reg=convOpAdjData(model,data,pef)\n",
    "eps=50\n",
    "\n",
    "problemPEF=ProblemL2LinearReg(model,data,j_op,eps,reg_op=reg)\n",
    "solve_base=LCGsolver(problemStop)\n",
    "solve_base.run(problemPEF)\n",
    "\n",
    "Grey(problemLap1.model,pclip=99)+Grey(problemPEF.model,pclip=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4982eb-99a0-4a44-9925-754195d82746",
   "metadata": {},
   "source": [
    "## Incorporating variance\n",
    "Least squares give us the minimum energy solution. As a result, it will tend towards\n",
    "zero most of the time (depending on boundary conditions). The solutions also tend to go\n",
    "quite smoothly as we move away from known data locations. In some cases this is a wanted\n",
    "results. Often it is not.\n",
    "\n",
    "\n",
    "## Krigging\n",
    " In the field of geostastics, the need for introducing morerealistic texture led to the concept of krigging. \n",
    "\n",
    "Krigging involves introducing some randomness to an estimate at a given location, not just was what is the best fit given are limited descriptions of the covariance. The basic \n",
    "approach to krigging is to randomly select points in the model space and find its nearest neighbors than based on a predefined level of variance and the covariance description choose from a distribution a value for that point. That now becomes a known point and the\n",
    "process is continued until every point has been visited.\n",
    "\n",
    "Assuming you change your random seed you can produce many equi-probable models that have more realistic texture than the non-random approach.  Averaging these will also give you\n",
    "the minimum energy solution.\n",
    "\n",
    "\n",
    "## Global least square and variance\n",
    "\n",
    "To apporach the problem from a global least squares problem is a little different.  The key is to consider are residuals.  Specifically lets look at the residual of our model styling goal. Look at the plot below which is the residuals of our Laplcian and PEF based regularized problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014abda-7cac-4e27-be26-8464a50a369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib')\n",
    "\n",
    "Grey(problemLap1.res.vecs[1],pclip=100)+Grey(problemPEF.res.vecs[1],pclip=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e653e0-e837-4fc3-b934-6c29e5e89ac7",
   "metadata": {},
   "source": [
    "In the case of the Laplacian problem the residuals are obviously not independent, we can see the obvious linear, dipping trend. In the case of the PEF they are much more IID.  Assuming that we have captured the covariance correctly, what do the amplitudes of the residual represent at locations where have know data?  \n",
    "\n",
    "A good assumption is that they cary information about other order of stastics, most likely dominated by first-order statistics and the variance.  THis leads to an interesting idea. What happens if we measure those stastics in some way and then somehow put them every in our residual? \n",
    "\n",
    "Specifically lets introduce random numbers to are model styling goal and change are minimization to $\\bf O(\\bf f) = || \\bf d -\\bf J \\bf m||^2 + \\epsilon^2 || \\bf r -\\bf A \\bf m||^2$ where $\\bf r$ are random numbers with approximately the same distribution as the residual at locations where we had known data.\n",
    "\n",
    "Lets first figure out a good range for are random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0c9fc-46b0-4966-aa95-0f5399fcfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_reg=problemPEF.res.vecs[1]\n",
    "res_reg.multiply(wt)\n",
    "tot=np.sum(np.abs(res_reg.get_nd_array()))\n",
    "nzero=np.sum(wt.get_nd_array())\n",
    "avg=tot/nzero/eps\n",
    "rnd=res_reg.clone()\n",
    "rnd.rand()\n",
    "rnd.scale(avg*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f8846-08ef-498a-8831-bf8dc039ec38",
   "metadata": {},
   "source": [
    "Lets then resolve are regularized inversion problem adding in those random nub,er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf43e55-cf2c-47c6-bfc1-4ac99c359679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib')\n",
    "\n",
    "j_op=DiagonalOp(unknown)\n",
    "problemStop=BasicStopper(niter=1000)\n",
    "data=vec.clone()\n",
    "model=data.clone()\n",
    "reg=convOpAdjData(model,data,pef)\n",
    "eps=50\n",
    "\n",
    "problemRand1=ProblemL2LinearReg(model,data,j_op,eps,reg_op=reg,prior_model=rnd)\n",
    "solve_base=LCGsolver(problemStop)\n",
    "solve_base.run(problemRand1)\n",
    "\n",
    "Grey(problemRand1.model,pclip=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8503a-8bd8-4246-96b7-77e965193871",
   "metadata": {},
   "source": [
    "We can redo the problem and get a different, realistic answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7419a6-dee4-48f8-9a77-5f1209198592",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib')\n",
    "\n",
    "rnd.rand()\n",
    "rnd.scale(avg*2.)\n",
    "problemRand2=ProblemL2LinearReg(model,data,j_op,eps,reg_op=reg,prior_model=rnd)\n",
    "solve_base=LCGsolver(problemStop)\n",
    "solve_base.run(problemRand2)\n",
    "\n",
    "Grey(problemRand2.model,pclip=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56953397-c825-4f90-918f-d57b3637bf84",
   "metadata": {},
   "source": [
    "If we attempt the same approach where we have not done a good job capturing the covariance the approach stil is interesting but much less satisfying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3f897-485f-4607-863b-80e7d048dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib')\n",
    "\n",
    "res_reg=problemLap1.res.vecs[1]\n",
    "res_reg.multiply(wt)\n",
    "tot=np.sum(np.abs(res_reg.get_nd_array()))\n",
    "nzero=np.sum(wt.get_nd_array())\n",
    "avg=tot/nzero/eps\n",
    "rnd=res_reg.clone()\n",
    "rnd.rand()\n",
    "rnd.scale(avg*2)\n",
    "hv.extension('bokeh','matplotlib')\n",
    "reg=Lap2D(vec,vec)\n",
    "j_op=DiagonalOp(unknown)\n",
    "problemStop=BasicStopper(niter=1000)\n",
    "data=vec.clone()\n",
    "model=data.clone()\n",
    "eps=50\n",
    "\n",
    "problemRand3=ProblemL2LinearReg(model,data,j_op,eps,reg_op=reg,prior_model=rnd)\n",
    "solve_base=LCGsolver(problemStop)\n",
    "solve_base.run(problemRand3)\n",
    "\n",
    "Grey(problemRand3.model,pclip=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889efbad-b75b-4491-ad87-599ec5a9fc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
